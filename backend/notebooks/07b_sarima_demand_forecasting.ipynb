{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07b - SARIMA Demand Forecasting Model\n",
    "\n",
    "## Business Objective\n",
    "Build a production-ready demand forecasting system using SARIMA to:\n",
    "- Predict future demand by product category\n",
    "- Capture weekly and monthly seasonal patterns\n",
    "- Support inventory planning with 30/60/90 day forecasts\n",
    "\n",
    "## Approach\n",
    "- Aggregate demand to daily level by category\n",
    "- Use SARIMA (Seasonal ARIMA) for time series forecasting\n",
    "- 80/20 time-based train-test split\n",
    "- Evaluate with MAPE, MAE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical/ML libraries\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path('../..') / 'ml' / 'data' / 'processed'\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD RAW DATA\n",
    "# Business Logic: We need order dates, quantities, and categories to\n",
    "# forecast demand by product category\n",
    "# ============================================================================\n",
    "\n",
    "# Load datasets\n",
    "orders = pd.read_csv(DATA_DIR / 'orders.csv')\n",
    "order_items = pd.read_csv(DATA_DIR / 'order_items.csv')\n",
    "products = pd.read_csv(DATA_DIR / 'products.csv')\n",
    "\n",
    "# Parse dates\n",
    "orders['OrderDate'] = pd.to_datetime(orders['OrderDate'])\n",
    "\n",
    "# Merge relevant columns\n",
    "# Business Logic: Join orders with items to get quantities, then with products to get categories\n",
    "df = orders[['OrderID', 'OrderDate', 'OrderStatus']].merge(\n",
    "    order_items[['OrderID', 'ProductID', 'Quantity']], on='OrderID'\n",
    ").merge(\n",
    "    products[['ProductID', 'Category']], on='ProductID'\n",
    ")\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df['OrderDate'].min().date()} to {df['OrderDate'].max().date()}\")\n",
    "print(f\"\\nCategories available: {df['Category'].nunique()}\")\n",
    "print(df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# Business Logic: Remove cancelled orders - they represent demand that\n",
    "# didn't actually occur and would skew our forecasts\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Order Status Distribution:\")\n",
    "print(df['OrderStatus'].value_counts())\n",
    "\n",
    "# Filter out cancelled orders\n",
    "df_clean = df[df['OrderStatus'] != 'Cancelled'].copy()\n",
    "print(f\"\\nRecords after removing cancelled: {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE TO DAILY DEMAND BY CATEGORY\n",
    "# Business Logic: Forecasting works best with regular time intervals.\n",
    "# We aggregate to daily totals per category for consistent time series.\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_category_data(df, category):\n",
    "    \"\"\"\n",
    "    Prepare daily demand time series for a specific category.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Filter to the selected category\n",
    "    - Sum quantities by date (daily aggregation)\n",
    "    - Fill missing dates with 0 (days with no orders)\n",
    "    - Set proper datetime index for time series analysis\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OrderDate, Quantity, Category columns\n",
    "        category: Category name to filter\n",
    "    \n",
    "    Returns:\n",
    "        Series with daily demand indexed by date\n",
    "    \"\"\"\n",
    "    # Filter to category\n",
    "    cat_df = df[df['Category'] == category].copy()\n",
    "    \n",
    "    # Aggregate to daily demand\n",
    "    daily = cat_df.groupby(cat_df['OrderDate'].dt.date)['Quantity'].sum()\n",
    "    daily.index = pd.to_datetime(daily.index)\n",
    "    \n",
    "    # Create complete date range and fill missing days with 0\n",
    "    # Business Logic: Days with no orders still represent 0 demand\n",
    "    date_range = pd.date_range(start=daily.index.min(), end=daily.index.max(), freq='D')\n",
    "    daily = daily.reindex(date_range, fill_value=0)\n",
    "    daily.index.name = 'Date'\n",
    "    daily.name = 'Quantity'\n",
    "    \n",
    "    return daily\n",
    "\n",
    "# Prepare data for all categories\n",
    "categories = df_clean['Category'].unique()\n",
    "print(f\"Categories: {list(categories)}\\n\")\n",
    "\n",
    "# Example: Show data for top category\n",
    "top_category = df_clean.groupby('Category')['Quantity'].sum().idxmax()\n",
    "sample_data = prepare_category_data(df_clean, top_category)\n",
    "print(f\"Sample data for '{top_category}':\")\n",
    "print(f\"  Date range: {sample_data.index.min().date()} to {sample_data.index.max().date()}\")\n",
    "print(f\"  Total days: {len(sample_data)}\")\n",
    "print(f\"  Mean daily demand: {sample_data.mean():.2f}\")\n",
    "print(f\"  Std daily demand: {sample_data.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STATIONARITY CHECK\n",
    "# Business Logic: SARIMA requires stationary data (constant mean/variance).\n",
    "# If non-stationary, we need differencing (d parameter in SARIMA).\n",
    "# ============================================================================\n",
    "\n",
    "def check_stationarity(series, name='Series'):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Stationary data has constant statistical properties over time\n",
    "    - Non-stationary data needs differencing before modeling\n",
    "    - p-value < 0.05 indicates stationarity\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        name: Name for display\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if series is stationary\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna(), autolag='AIC')\n",
    "    \n",
    "    print(f\"\\nStationarity Test for {name}:\")\n",
    "    print(f\"  ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"  p-value: {result[1]:.6f}\")\n",
    "    print(f\"  Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"    {key}: {value:.4f}\")\n",
    "    \n",
    "    is_stationary = result[1] < 0.05\n",
    "    print(f\"  Result: {'✓ STATIONARY' if is_stationary else '✗ NON-STATIONARY'}\")\n",
    "    \n",
    "    return is_stationary\n",
    "\n",
    "# Test on sample data\n",
    "is_stationary = check_stationarity(sample_data, top_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# Business Logic: For time series, we must split chronologically.\n",
    "# We use 80% for training and 20% for testing to evaluate real-world performance.\n",
    "# ============================================================================\n",
    "\n",
    "def train_test_split_ts(series, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split time series data into train and test sets.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Time series MUST be split chronologically (not randomly)\n",
    "    - We train on historical data and test on future data\n",
    "    - This simulates real-world forecasting scenarios\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        train_ratio: Proportion for training (default 80%)\n",
    "    \n",
    "    Returns:\n",
    "        train, test series\n",
    "    \"\"\"\n",
    "    split_idx = int(len(series) * train_ratio)\n",
    "    train = series[:split_idx]\n",
    "    test = series[split_idx:]\n",
    "    \n",
    "    print(f\"Train-Test Split:\")\n",
    "    print(f\"  Training: {len(train)} days ({train.index.min().date()} to {train.index.max().date()})\")\n",
    "    print(f\"  Testing: {len(test)} days ({test.index.min().date()} to {test.index.max().date()})\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Split sample data\n",
    "train, test = train_test_split_ts(sample_data, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. SARIMA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SARIMA MODEL CLASS\n",
    "# Business Logic: SARIMA captures both non-seasonal and seasonal patterns.\n",
    "# For retail demand:\n",
    "#   - Weekly seasonality (s=7): Weekend vs weekday patterns\n",
    "#   - Monthly patterns captured through longer seasonal periods\n",
    "# ============================================================================\n",
    "\n",
    "class SARIMAForecaster:\n",
    "    \"\"\"\n",
    "    Production-ready SARIMA forecasting model for demand prediction.\n",
    "    \n",
    "    SARIMA(p, d, q)(P, D, Q, s) parameters:\n",
    "    - p: Autoregressive order (how many past values influence current)\n",
    "    - d: Differencing order (how many times to difference for stationarity)\n",
    "    - q: Moving average order (how many past errors influence current)\n",
    "    - P: Seasonal autoregressive order\n",
    "    - D: Seasonal differencing order\n",
    "    - Q: Seasonal moving average order\n",
    "    - s: Seasonal period (7 for weekly, 30 for monthly)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seasonal_period=7):\n",
    "        \"\"\"\n",
    "        Initialize forecaster.\n",
    "        \n",
    "        Args:\n",
    "            seasonal_period: Period for seasonality (default 7 for weekly)\n",
    "        \"\"\"\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.order = None\n",
    "        self.seasonal_order = None\n",
    "        \n",
    "    def find_best_parameters(self, train_data, p_range=(0, 2), d_range=(0, 2), \n",
    "                             q_range=(0, 2), P_range=(0, 1), D_range=(0, 1), Q_range=(0, 1)):\n",
    "        \"\"\"\n",
    "        Find optimal SARIMA parameters using AIC criterion.\n",
    "        \n",
    "        Business Logic:\n",
    "        - AIC (Akaike Information Criterion) balances model fit vs complexity\n",
    "        - Lower AIC indicates better model\n",
    "        - We test multiple parameter combinations\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training time series\n",
    "            p_range, d_range, q_range: Non-seasonal parameter ranges\n",
    "            P_range, D_range, Q_range: Seasonal parameter ranges\n",
    "        \n",
    "        Returns:\n",
    "            Best (order, seasonal_order) tuple\n",
    "        \"\"\"\n",
    "        best_aic = float('inf')\n",
    "        best_params = None\n",
    "        results = []\n",
    "        \n",
    "        print(\"Searching for best SARIMA parameters...\")\n",
    "        \n",
    "        for p in range(p_range[0], p_range[1] + 1):\n",
    "            for d in range(d_range[0], d_range[1] + 1):\n",
    "                for q in range(q_range[0], q_range[1] + 1):\n",
    "                    for P in range(P_range[0], P_range[1] + 1):\n",
    "                        for D in range(D_range[0], D_range[1] + 1):\n",
    "                            for Q in range(Q_range[0], Q_range[1] + 1):\n",
    "                                try:\n",
    "                                    order = (p, d, q)\n",
    "                                    seasonal_order = (P, D, Q, self.seasonal_period)\n",
    "                                    \n",
    "                                    model = SARIMAX(\n",
    "                                        train_data,\n",
    "                                        order=order,\n",
    "                                        seasonal_order=seasonal_order,\n",
    "                                        enforce_stationarity=False,\n",
    "                                        enforce_invertibility=False\n",
    "                                    )\n",
    "                                    fitted = model.fit(disp=False, maxiter=100)\n",
    "                                    \n",
    "                                    results.append({\n",
    "                                        'order': order,\n",
    "                                        'seasonal_order': seasonal_order,\n",
    "                                        'aic': fitted.aic\n",
    "                                    })\n",
    "                                    \n",
    "                                    if fitted.aic < best_aic:\n",
    "                                        best_aic = fitted.aic\n",
    "                                        best_params = (order, seasonal_order)\n",
    "                                        \n",
    "                                except Exception as e:\n",
    "                                    continue\n",
    "        \n",
    "        print(f\"\\nBest parameters found:\")\n",
    "        print(f\"  Order: {best_params[0]}\")\n",
    "        print(f\"  Seasonal Order: {best_params[1]}\")\n",
    "        print(f\"  AIC: {best_aic:.2f}\")\n",
    "        \n",
    "        self.order = best_params[0]\n",
    "        self.seasonal_order = best_params[1]\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def fit(self, train_data, order=None, seasonal_order=None):\n",
    "        \"\"\"\n",
    "        Fit SARIMA model to training data.\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training time series\n",
    "            order: (p, d, q) tuple or None to use auto-found\n",
    "            seasonal_order: (P, D, Q, s) tuple or None to use auto-found\n",
    "        \n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        if order is not None:\n",
    "            self.order = order\n",
    "        if seasonal_order is not None:\n",
    "            self.seasonal_order = seasonal_order\n",
    "            \n",
    "        if self.order is None or self.seasonal_order is None:\n",
    "            raise ValueError(\"Must set parameters via find_best_parameters() or provide them directly\")\n",
    "        \n",
    "        print(f\"Fitting SARIMA{self.order}x{self.seasonal_order}...\")\n",
    "        \n",
    "        self.model = SARIMAX(\n",
    "            train_data,\n",
    "            order=self.order,\n",
    "            seasonal_order=self.seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        self.fitted_model = self.model.fit(disp=False, maxiter=200)\n",
    "        \n",
    "        print(f\"Model fitted successfully\")\n",
    "        print(f\"  AIC: {self.fitted_model.aic:.2f}\")\n",
    "        print(f\"  BIC: {self.fitted_model.bic:.2f}\")\n",
    "        \n",
    "        return self.fitted_model\n",
    "    \n",
    "    def forecast(self, steps, return_conf_int=True, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Generate forecasts for future periods.\n",
    "        \n",
    "        Business Logic:\n",
    "        - Forecasts include confidence intervals for uncertainty quantification\n",
    "        - 95% confidence interval (alpha=0.05) is standard for planning\n",
    "        \n",
    "        Args:\n",
    "            steps: Number of periods to forecast\n",
    "            return_conf_int: Whether to return confidence intervals\n",
    "            alpha: Significance level for confidence intervals\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with forecast, lower_ci, upper_ci columns\n",
    "        \"\"\"\n",
    "        if self.fitted_model is None:\n",
    "            raise ValueError(\"Must fit model before forecasting\")\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast_result = self.fitted_model.get_forecast(steps=steps)\n",
    "        forecast_mean = forecast_result.predicted_mean\n",
    "        conf_int = forecast_result.conf_int(alpha=alpha)\n",
    "        \n",
    "        # Create output DataFrame\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'Forecast': forecast_mean.values,\n",
    "            'Lower_CI': conf_int.iloc[:, 0].values,\n",
    "            'Upper_CI': conf_int.iloc[:, 1].values\n",
    "        }, index=forecast_mean.index)\n",
    "        \n",
    "        # Business Logic: Demand cannot be negative\n",
    "        forecast_df['Forecast'] = forecast_df['Forecast'].clip(lower=0)\n",
    "        forecast_df['Lower_CI'] = forecast_df['Lower_CI'].clip(lower=0)\n",
    "        \n",
    "        return forecast_df\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test data.\n",
    "        \n",
    "        Business Logic:\n",
    "        - MAE: Average absolute error (interpretable in units)\n",
    "        - RMSE: Penalizes large errors more heavily\n",
    "        - MAPE: Percentage error (scale-independent)\n",
    "        \n",
    "        Args:\n",
    "            test_data: Actual test series\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metrics\n",
    "        \"\"\"\n",
    "        # Generate predictions for test period\n",
    "        forecast_df = self.forecast(steps=len(test_data))\n",
    "        predictions = forecast_df['Forecast'].values\n",
    "        actual = test_data.values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(actual, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "        \n",
    "        # MAPE: Handle zeros in actual values\n",
    "        mask = actual != 0\n",
    "        if mask.sum() > 0:\n",
    "            mape = np.mean(np.abs((actual[mask] - predictions[mask]) / actual[mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        metrics = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "        \n",
    "        return metrics, forecast_df\n",
    "    \n",
    "    def check_residuals(self):\n",
    "        \"\"\"\n",
    "        Perform residual diagnostics.\n",
    "        \n",
    "        Business Logic:\n",
    "        - Good model should have random residuals (no patterns)\n",
    "        - Residuals should be normally distributed\n",
    "        - No autocorrelation in residuals\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with diagnostic test results\n",
    "        \"\"\"\n",
    "        if self.fitted_model is None:\n",
    "            raise ValueError(\"Must fit model first\")\n",
    "        \n",
    "        residuals = self.fitted_model.resid\n",
    "        \n",
    "        # Ljung-Box test for autocorrelation\n",
    "        lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "        lb_pvalue = lb_test['lb_pvalue'].values[0]\n",
    "        \n",
    "        # Normality test\n",
    "        jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
    "        \n",
    "        diagnostics = {\n",
    "            'residual_mean': residuals.mean(),\n",
    "            'residual_std': residuals.std(),\n",
    "            'ljung_box_pvalue': lb_pvalue,\n",
    "            'no_autocorrelation': lb_pvalue > 0.05,\n",
    "            'jarque_bera_pvalue': jb_pvalue,\n",
    "            'normality': jb_pvalue > 0.05\n",
    "        }\n",
    "        \n",
    "        return diagnostics, residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# Business Logic: Visualizations help stakeholders understand forecasts\n",
    "# and build confidence in the model\n",
    "# ============================================================================\n",
    "\n",
    "def plot_actual_vs_predicted(actual, predicted, title='Actual vs Predicted'):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Shows how well the model captures actual patterns\n",
    "    - Helps identify periods where model under/over predicts\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    ax.plot(actual.index, actual.values, label='Actual', linewidth=2, color='blue')\n",
    "    ax.plot(predicted.index, predicted.values, label='Predicted', linewidth=2, \n",
    "            color='red', linestyle='--')\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Quantity')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_forecast_with_ci(train, test, forecast_df, title='Demand Forecast'):\n",
    "    \"\"\"\n",
    "    Plot forecast with confidence intervals.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Confidence intervals show forecast uncertainty\n",
    "    - Wider intervals = more uncertainty\n",
    "    - Helps with safety stock planning (use upper CI for buffer)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Plot training data (last 90 days for clarity)\n",
    "    train_plot = train[-90:] if len(train) > 90 else train\n",
    "    ax.plot(train_plot.index, train_plot.values, label='Historical', \n",
    "            linewidth=2, color='blue')\n",
    "    \n",
    "    # Plot test data (actual)\n",
    "    if test is not None:\n",
    "        ax.plot(test.index, test.values, label='Actual', \n",
    "                linewidth=2, color='green')\n",
    "    \n",
    "    # Plot forecast\n",
    "    ax.plot(forecast_df.index, forecast_df['Forecast'], \n",
    "            label='Forecast', linewidth=2, color='red')\n",
    "    \n",
    "    # Plot confidence interval\n",
    "    ax.fill_between(forecast_df.index, \n",
    "                    forecast_df['Lower_CI'], \n",
    "                    forecast_df['Upper_CI'],\n",
    "                    color='red', alpha=0.2, label='95% CI')\n",
    "    \n",
    "    # Add vertical line at forecast start\n",
    "    ax.axvline(x=forecast_df.index[0], color='gray', linestyle=':', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Quantity')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_residual_diagnostics(residuals, title='Residual Diagnostics'):\n",
    "    \"\"\"\n",
    "    Plot residual diagnostic charts.\n",
    "    \n",
    "    Business Logic:\n",
    "    - Residuals should look like random noise\n",
    "    - Patterns indicate model is missing something\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Time series of residuals\n",
    "    axes[0, 0].plot(residuals)\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 0].set_title('Residuals Over Time')\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('Residual')\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 1].hist(residuals, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "    xmin, xmax = axes[0, 1].get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    axes[0, 1].plot(x, stats.norm.pdf(x, residuals.mean(), residuals.std()), 'r-', linewidth=2)\n",
    "    axes[0, 1].set_title('Residual Distribution')\n",
    "    axes[0, 1].set_xlabel('Residual')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot')\n",
    "    \n",
    "    # ACF of residuals\n",
    "    plot_acf(residuals, ax=axes[1, 1], lags=30)\n",
    "    axes[1, 1].set_title('ACF of Residuals')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Run Forecasting for a Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FULL FORECASTING PIPELINE FOR ONE CATEGORY\n",
    "# ============================================================================\n",
    "\n",
    "def run_forecast_pipeline(df, category, forecast_horizons=[30, 60, 90]):\n",
    "    \"\"\"\n",
    "    Complete forecasting pipeline for a category.\n",
    "    \n",
    "    Business Logic:\n",
    "    - 30 days: Short-term operational planning\n",
    "    - 60 days: Medium-term inventory planning\n",
    "    - 90 days: Long-term strategic planning\n",
    "    \n",
    "    Args:\n",
    "        df: Clean DataFrame with OrderDate, Quantity, Category\n",
    "        category: Category to forecast\n",
    "        forecast_horizons: List of forecast periods\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model, metrics, and forecasts\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FORECASTING: {category}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    print(\"\\n[Step 1] Preparing data...\")\n",
    "    data = prepare_category_data(df, category)\n",
    "    \n",
    "    # Step 2: Check stationarity\n",
    "    print(\"\\n[Step 2] Checking stationarity...\")\n",
    "    is_stationary = check_stationarity(data, category)\n",
    "    \n",
    "    # Step 3: Train-test split (80/20)\n",
    "    print(\"\\n[Step 3] Splitting data...\")\n",
    "    train, test = train_test_split_ts(data, train_ratio=0.8)\n",
    "    \n",
    "    # Step 4: Initialize and fit model\n",
    "    print(\"\\n[Step 4] Building SARIMA model...\")\n",
    "    forecaster = SARIMAForecaster(seasonal_period=7)  # Weekly seasonality\n",
    "    \n",
    "    # Use default good parameters to save time (you can enable grid search)\n",
    "    # forecaster.find_best_parameters(train)\n",
    "    forecaster.fit(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
    "    \n",
    "    # Step 5: Evaluate on test set\n",
    "    print(\"\\n[Step 5] Evaluating model...\")\n",
    "    metrics, test_forecast = forecaster.evaluate(test)\n",
    "    \n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  MAE:  {metrics['MAE']:.2f} units\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f} units\")\n",
    "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    \n",
    "    # Step 6: Residual diagnostics\n",
    "    print(\"\\n[Step 6] Checking residuals...\")\n",
    "    diagnostics, residuals = forecaster.check_residuals()\n",
    "    \n",
    "    print(f\"  Residual Mean: {diagnostics['residual_mean']:.4f} (should be ~0)\")\n",
    "    print(f\"  No Autocorrelation: {'✓ PASS' if diagnostics['no_autocorrelation'] else '✗ FAIL'}\")\n",
    "    print(f\"  Normality: {'✓ PASS' if diagnostics['normality'] else '✗ FAIL (acceptable)'}\")\n",
    "    \n",
    "    # Step 7: Generate future forecasts\n",
    "    print(\"\\n[Step 7] Generating forecasts...\")\n",
    "    \n",
    "    # Refit on full data for production forecasts\n",
    "    forecaster.fit(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
    "    \n",
    "    forecasts = {}\n",
    "    for horizon in forecast_horizons:\n",
    "        forecast_df = forecaster.forecast(steps=horizon)\n",
    "        forecasts[f'{horizon}_day'] = forecast_df\n",
    "        total_forecast = forecast_df['Forecast'].sum()\n",
    "        print(f\"  {horizon}-day forecast: {total_forecast:.0f} total units\")\n",
    "    \n",
    "    return {\n",
    "        'category': category,\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'test_forecast': test_forecast,\n",
    "        'metrics': metrics,\n",
    "        'diagnostics': diagnostics,\n",
    "        'residuals': residuals,\n",
    "        'forecasts': forecasts,\n",
    "        'model': forecaster\n",
    "    }\n",
    "\n",
    "# Run for top category\n",
    "results = run_forecast_pipeline(df_clean, top_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Actual vs Predicted on test set\n",
    "test_actual = results['test']\n",
    "test_pred = results['test_forecast']['Forecast']\n",
    "test_pred.index = test_actual.index  # Align indices\n",
    "\n",
    "fig1 = plot_actual_vs_predicted(\n",
    "    test_actual, \n",
    "    test_pred,\n",
    "    title=f\"Actual vs Predicted - {results['category']} (Test Period)\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 2. Forecast with confidence intervals (90-day)\n",
    "fig2 = plot_forecast_with_ci(\n",
    "    results['train'],\n",
    "    results['test'],\n",
    "    results['forecasts']['90_day'],\n",
    "    title=f\"90-Day Demand Forecast - {results['category']}\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 3. Residual diagnostics\n",
    "fig3 = plot_residual_diagnostics(\n",
    "    results['residuals'],\n",
    "    title=f\"Residual Diagnostics - {results['category']}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Forecast All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN FORECASTING FOR ALL CATEGORIES\n",
    "# Business Logic: Generate forecasts for entire product portfolio\n",
    "# ============================================================================\n",
    "\n",
    "def forecast_all_categories(df, categories=None):\n",
    "    \"\"\"\n",
    "    Run forecasting pipeline for multiple categories.\n",
    "    \n",
    "    Args:\n",
    "        df: Clean DataFrame\n",
    "        categories: List of categories (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping category to results\n",
    "    \"\"\"\n",
    "    if categories is None:\n",
    "        categories = df['Category'].unique()\n",
    "    \n",
    "    all_results = {}\n",
    "    summary_data = []\n",
    "    \n",
    "    for category in categories:\n",
    "        try:\n",
    "            results = run_forecast_pipeline(df, category)\n",
    "            all_results[category] = results\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Category': category,\n",
    "                'MAE': results['metrics']['MAE'],\n",
    "                'RMSE': results['metrics']['RMSE'],\n",
    "                'MAPE': results['metrics']['MAPE'],\n",
    "                '30_Day_Forecast': results['forecasts']['30_day']['Forecast'].sum(),\n",
    "                '60_Day_Forecast': results['forecasts']['60_day']['Forecast'].sum(),\n",
    "                '90_Day_Forecast': results['forecasts']['90_day']['Forecast'].sum()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError forecasting {category}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return all_results, summary_df\n",
    "\n",
    "# Run for all categories\n",
    "all_results, summary_df = forecast_all_categories(df_clean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORECAST SUMMARY - ALL CATEGORIES\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. MAPE by Category\n",
    "summary_sorted = summary_df.sort_values('MAPE')\n",
    "axes[0, 0].barh(summary_sorted['Category'], summary_sorted['MAPE'])\n",
    "axes[0, 0].set_xlabel('MAPE (%)')\n",
    "axes[0, 0].set_title('Forecast Accuracy by Category (MAPE)')\n",
    "axes[0, 0].axvline(x=20, color='r', linestyle='--', label='20% threshold')\n",
    "\n",
    "# 2. 90-Day Forecast by Category\n",
    "summary_sorted = summary_df.sort_values('90_Day_Forecast', ascending=True)\n",
    "axes[0, 1].barh(summary_sorted['Category'], summary_sorted['90_Day_Forecast'])\n",
    "axes[0, 1].set_xlabel('Total Units')\n",
    "axes[0, 1].set_title('90-Day Demand Forecast by Category')\n",
    "\n",
    "# 3. MAE vs RMSE comparison\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, summary_df['MAE'], width, label='MAE')\n",
    "axes[1, 0].bar(x + width/2, summary_df['RMSE'], width, label='RMSE')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(summary_df['Category'], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Error (Units)')\n",
    "axes[1, 0].set_title('MAE vs RMSE by Category')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Forecast horizons comparison\n",
    "summary_df.plot(x='Category', y=['30_Day_Forecast', '60_Day_Forecast', '90_Day_Forecast'],\n",
    "               kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Forecast by Horizon')\n",
    "axes[1, 1].set_ylabel('Total Units')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend(['30-Day', '60-Day', '90-Day'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORT FORECAST RESULTS\n",
    "# Business Logic: Save forecasts for use in inventory planning systems\n",
    "# ============================================================================\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(DATA_DIR / 'forecast_summary.csv', index=False)\n",
    "print(f\"Saved forecast summary to: {DATA_DIR / 'forecast_summary.csv'}\")\n",
    "\n",
    "# Save detailed forecasts for each category\n",
    "for category, results in all_results.items():\n",
    "    category_clean = category.replace(' ', '_').replace('&', 'and')\n",
    "    \n",
    "    # Save 90-day forecast with confidence intervals\n",
    "    forecast_90 = results['forecasts']['90_day'].copy()\n",
    "    forecast_90['Category'] = category\n",
    "    forecast_90.to_csv(DATA_DIR / f'forecast_90day_{category_clean}.csv')\n",
    "\n",
    "print(f\"\\nDetailed forecasts saved for {len(all_results)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SARIMA DEMAND FORECASTING - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL SPECIFICATION:\n",
    "  Type: SARIMA(1,1,1)(1,1,1,7)\n",
    "  Seasonal Period: 7 (weekly)\n",
    "  Train/Test Split: 80/20 (time-based)\n",
    "\n",
    "OVERALL PERFORMANCE:\n",
    "  Average MAPE: {summary_df['MAPE'].mean():.2f}%\n",
    "  Average MAE: {summary_df['MAE'].mean():.2f} units\n",
    "  Average RMSE: {summary_df['RMSE'].mean():.2f} units\n",
    "\n",
    "BEST PERFORMING CATEGORY:\n",
    "  {summary_df.loc[summary_df['MAPE'].idxmin(), 'Category']} (MAPE: {summary_df['MAPE'].min():.2f}%)\n",
    "\n",
    "90-DAY TOTAL FORECAST:\n",
    "  {summary_df['90_Day_Forecast'].sum():.0f} units across all categories\n",
    "\n",
    "FILES GENERATED:\n",
    "  - forecast_summary.csv (all categories)\n",
    "  - forecast_90day_[category].csv (detailed forecasts)\n",
    "\n",
    "BUSINESS RECOMMENDATIONS:\n",
    "  1. Use forecast + Upper_CI for safety stock planning\n",
    "  2. Categories with MAPE > 25% need additional review\n",
    "  3. Re-run model monthly to incorporate new data\n",
    "  4. Monitor forecast accuracy and adjust as needed\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
