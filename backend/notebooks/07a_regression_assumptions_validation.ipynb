{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07a - Linear Regression Assumptions Validation\n",
    "\n",
    "## Purpose\n",
    "Validate that our demand forecasting data meets the assumptions required for Linear Regression and implement proper preprocessing strategies.\n",
    "\n",
    "## Linear Regression Assumptions\n",
    "1. **Linearity** - Linear relationship between features and target\n",
    "2. **Independence** - Observations are independent (no autocorrelation)\n",
    "3. **Homoscedasticity** - Constant variance of residuals\n",
    "4. **Normality** - Residuals are normally distributed\n",
    "5. **No Multicollinearity** - Features are not highly correlated\n",
    "\n",
    "## Additional Preprocessing\n",
    "- Outlier detection and handling\n",
    "- Feature scaling considerations\n",
    "- Data transformations if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Load data\n",
    "DATA_DIR = Path('../..') / 'ml' / 'data' / 'processed'\n",
    "\n",
    "orders = pd.read_csv(DATA_DIR / 'orders.csv')\n",
    "order_items = pd.read_csv(DATA_DIR / 'order_items.csv')\n",
    "products = pd.read_csv(DATA_DIR / 'products.csv')\n",
    "\n",
    "orders['OrderDate'] = pd.to_datetime(orders['OrderDate'])\n",
    "full_orders = orders.merge(order_items, on='OrderID').merge(products, on='ProductID')\n",
    "\n",
    "print(f\"Data loaded: {len(full_orders):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare daily demand data for a sample product\n",
    "daily_demand = full_orders.groupby([full_orders['OrderDate'].dt.date, 'ProductID'])['Quantity'].sum().reset_index()\n",
    "daily_demand.columns = ['Date', 'ProductID', 'Quantity']\n",
    "daily_demand['Date'] = pd.to_datetime(daily_demand['Date'])\n",
    "\n",
    "# Select top product for detailed analysis\n",
    "top_product = full_orders.groupby('ProductID')['Quantity'].sum().idxmax()\n",
    "product_data = daily_demand[daily_demand['ProductID'] == top_product].sort_values('Date').copy()\n",
    "\n",
    "# Fill missing dates with 0\n",
    "date_range = pd.date_range(product_data['Date'].min(), product_data['Date'].max())\n",
    "product_data = product_data.set_index('Date').reindex(date_range, fill_value=0).reset_index()\n",
    "product_data.columns = ['Date', 'ProductID', 'Quantity']\n",
    "product_data['ProductID'] = top_product\n",
    "\n",
    "# Create features\n",
    "product_data['DayIndex'] = (product_data['Date'] - product_data['Date'].min()).dt.days\n",
    "product_data['DayOfWeek'] = product_data['Date'].dt.dayofweek\n",
    "product_data['Month'] = product_data['Date'].dt.month\n",
    "product_data['WeekOfYear'] = product_data['Date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "print(f\"Analyzing product: {top_product}\")\n",
    "print(f\"Data points: {len(product_data)}\")\n",
    "print(f\"\\nTarget variable (Quantity) statistics:\")\n",
    "print(product_data['Quantity'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: IQR-based outlier detection\n",
    "Q1 = product_data['Quantity'].quantile(0.25)\n",
    "Q3 = product_data['Quantity'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = product_data[(product_data['Quantity'] < lower_bound) | (product_data['Quantity'] > upper_bound)]\n",
    "\n",
    "# Method 2: Z-score based outlier detection\n",
    "z_scores = np.abs(stats.zscore(product_data['Quantity']))\n",
    "outliers_zscore = product_data[z_scores > 3]\n",
    "\n",
    "# Method 3: Modified Z-score (robust to outliers)\n",
    "median = product_data['Quantity'].median()\n",
    "mad = np.median(np.abs(product_data['Quantity'] - median))\n",
    "modified_z = 0.6745 * (product_data['Quantity'] - median) / (mad if mad != 0 else 1)\n",
    "outliers_modified_z = product_data[np.abs(modified_z) > 3.5]\n",
    "\n",
    "print(\"=== OUTLIER DETECTION RESULTS ===\")\n",
    "print(f\"\\nIQR Method:\")\n",
    "print(f\"  Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"  Upper bound: {upper_bound:.2f}\")\n",
    "print(f\"  Outliers found: {len(outliers_iqr)} ({len(outliers_iqr)/len(product_data)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nZ-Score Method (threshold=3):\")\n",
    "print(f\"  Outliers found: {len(outliers_zscore)} ({len(outliers_zscore)/len(product_data)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nModified Z-Score Method (threshold=3.5):\")\n",
    "print(f\"  Outliers found: {len(outliers_modified_z)} ({len(outliers_modified_z)/len(product_data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Box plot\n",
    "axes[0, 0].boxplot(product_data['Quantity'])\n",
    "axes[0, 0].set_title('Box Plot - Quantity Distribution')\n",
    "axes[0, 0].set_ylabel('Quantity')\n",
    "\n",
    "# Histogram with normal curve\n",
    "axes[0, 1].hist(product_data['Quantity'], bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "xmin, xmax = axes[0, 1].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "axes[0, 1].plot(x, stats.norm.pdf(x, product_data['Quantity'].mean(), product_data['Quantity'].std()), 'r-', linewidth=2)\n",
    "axes[0, 1].set_title('Histogram with Normal Distribution Overlay')\n",
    "axes[0, 1].set_xlabel('Quantity')\n",
    "\n",
    "# Time series with outliers highlighted\n",
    "axes[1, 0].plot(product_data['Date'], product_data['Quantity'], alpha=0.7)\n",
    "axes[1, 0].scatter(outliers_iqr['Date'], outliers_iqr['Quantity'], color='red', s=50, label='Outliers (IQR)')\n",
    "axes[1, 0].axhline(y=upper_bound, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axhline(y=lower_bound, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_title('Time Series with Outliers Highlighted')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(product_data['Quantity'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Handling Strategies\n",
    "print(\"=== OUTLIER HANDLING STRATEGIES ===\")\n",
    "\n",
    "# Strategy 1: Remove outliers\n",
    "data_no_outliers = product_data[(product_data['Quantity'] >= lower_bound) & (product_data['Quantity'] <= upper_bound)].copy()\n",
    "\n",
    "# Strategy 2: Cap/Winsorize outliers\n",
    "data_capped = product_data.copy()\n",
    "data_capped['Quantity'] = data_capped['Quantity'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Strategy 3: Log transformation (for right-skewed data)\n",
    "data_log = product_data.copy()\n",
    "data_log['Quantity_Log'] = np.log1p(data_log['Quantity'])  # log(1+x) handles zeros\n",
    "\n",
    "# Strategy 4: Use robust scaler\n",
    "robust_scaler = RobustScaler()\n",
    "data_robust = product_data.copy()\n",
    "data_robust['Quantity_Scaled'] = robust_scaler.fit_transform(data_robust[['Quantity']])\n",
    "\n",
    "print(f\"\\nOriginal data: {len(product_data)} records\")\n",
    "print(f\"After removing outliers: {len(data_no_outliers)} records ({len(product_data) - len(data_no_outliers)} removed)\")\n",
    "print(f\"\\nSkewness:\")\n",
    "print(f\"  Original: {product_data['Quantity'].skew():.3f}\")\n",
    "print(f\"  After capping: {data_capped['Quantity'].skew():.3f}\")\n",
    "print(f\"  After log transform: {data_log['Quantity_Log'].skew():.3f}\")\n",
    "\n",
    "# Recommendation based on skewness\n",
    "skewness = product_data['Quantity'].skew()\n",
    "if abs(skewness) > 1:\n",
    "    print(f\"\\n⚠️ Data is highly skewed ({skewness:.3f}). Consider log transformation.\")\n",
    "elif abs(skewness) > 0.5:\n",
    "    print(f\"\\n⚠️ Data is moderately skewed ({skewness:.3f}). Consider capping outliers.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Data skewness is acceptable ({skewness:.3f}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Assumption 1: Linearity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check linearity between features and target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# DayIndex vs Quantity\n",
    "axes[0, 0].scatter(product_data['DayIndex'], product_data['Quantity'], alpha=0.5)\n",
    "z = np.polyfit(product_data['DayIndex'], product_data['Quantity'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(product_data['DayIndex'], p(product_data['DayIndex']), 'r-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Day Index')\n",
    "axes[0, 0].set_ylabel('Quantity')\n",
    "axes[0, 0].set_title('Linearity Check: DayIndex vs Quantity')\n",
    "\n",
    "# DayOfWeek vs Quantity (categorical - use box plot)\n",
    "product_data.boxplot(column='Quantity', by='DayOfWeek', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Quantity by Day of Week')\n",
    "axes[0, 1].set_xlabel('Day of Week (0=Mon, 6=Sun)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Month vs Quantity\n",
    "product_data.boxplot(column='Quantity', by='Month', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Quantity by Month')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_cols = ['DayIndex', 'DayOfWeek', 'Month', 'Quantity']\n",
    "corr_matrix = product_data[corr_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "print(\"=== LINEARITY ASSESSMENT ===\")\n",
    "for col in ['DayIndex', 'DayOfWeek', 'Month']:\n",
    "    corr, p_value = stats.pearsonr(product_data[col], product_data['Quantity'])\n",
    "    print(f\"{col}: r = {corr:.4f}, p-value = {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  → Statistically significant linear relationship\")\n",
    "    else:\n",
    "        print(f\"  → No significant linear relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Fit Model and Check Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression model\n",
    "X = product_data[['DayIndex', 'DayOfWeek', 'Month']]\n",
    "y = product_data['Quantity']\n",
    "\n",
    "# Add constant for statsmodels\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "# Get predictions and residuals\n",
    "predictions = model.predict(X_const)\n",
    "residuals = y - predictions\n",
    "\n",
    "print(\"=== MODEL SUMMARY ===\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Assumption 2: Independence (No Autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durbin-Watson test for autocorrelation\n",
    "dw_statistic = durbin_watson(residuals)\n",
    "\n",
    "print(\"=== INDEPENDENCE CHECK (Durbin-Watson Test) ===\")\n",
    "print(f\"Durbin-Watson statistic: {dw_statistic:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  DW ≈ 2: No autocorrelation (ideal)\")\n",
    "print(f\"  DW < 2: Positive autocorrelation\")\n",
    "print(f\"  DW > 2: Negative autocorrelation\")\n",
    "\n",
    "if 1.5 <= dw_statistic <= 2.5:\n",
    "    print(f\"\\n✓ Durbin-Watson = {dw_statistic:.4f} is within acceptable range [1.5, 2.5]\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Durbin-Watson = {dw_statistic:.4f} indicates autocorrelation issue\")\n",
    "    print(\"   Consider: Adding lag features, using time-series specific models (ARIMA)\")\n",
    "\n",
    "# Plot residuals over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(product_data['Date'], residuals)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_title('Residuals Over Time')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "\n",
    "# Autocorrelation plot\n",
    "pd.plotting.autocorrelation_plot(residuals, ax=axes[1])\n",
    "axes[1].set_title('Autocorrelation Plot of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Assumption 3: Homoscedasticity (Constant Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breusch-Pagan test for heteroscedasticity\n",
    "bp_test = het_breuschpagan(residuals, X_const)\n",
    "bp_statistic, bp_pvalue, bp_f_stat, bp_f_pvalue = bp_test\n",
    "\n",
    "print(\"=== HOMOSCEDASTICITY CHECK (Breusch-Pagan Test) ===\")\n",
    "print(f\"Lagrange Multiplier statistic: {bp_statistic:.4f}\")\n",
    "print(f\"p-value: {bp_pvalue:.4f}\")\n",
    "print(f\"F-statistic: {bp_f_stat:.4f}\")\n",
    "print(f\"F p-value: {bp_f_pvalue:.4f}\")\n",
    "\n",
    "if bp_pvalue > 0.05:\n",
    "    print(f\"\\n✓ p-value = {bp_pvalue:.4f} > 0.05: Homoscedasticity assumption holds\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ p-value = {bp_pvalue:.4f} < 0.05: Heteroscedasticity detected\")\n",
    "    print(\"   Consider: Weighted Least Squares, log transformation, robust standard errors\")\n",
    "\n",
    "# Visual check: Residuals vs Fitted values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(predictions, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Fitted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residuals vs Fitted Values (Homoscedasticity Check)')\n",
    "\n",
    "# Scale-Location plot\n",
    "axes[1].scatter(predictions, np.sqrt(np.abs(residuals)), alpha=0.5)\n",
    "axes[1].set_xlabel('Fitted Values')\n",
    "axes[1].set_ylabel('√|Residuals|')\n",
    "axes[1].set_title('Scale-Location Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Assumption 4: Normality of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk test for normality (use sample if data is large)\n",
    "sample_size = min(5000, len(residuals))\n",
    "residual_sample = residuals.sample(sample_size) if len(residuals) > 5000 else residuals\n",
    "shapiro_stat, shapiro_pvalue = stats.shapiro(residual_sample)\n",
    "\n",
    "# Jarque-Bera test\n",
    "jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
    "\n",
    "# D'Agostino-Pearson test\n",
    "dagostino_stat, dagostino_pvalue = stats.normaltest(residuals)\n",
    "\n",
    "print(\"=== NORMALITY CHECK ===\")\n",
    "print(f\"\\nShapiro-Wilk Test:\")\n",
    "print(f\"  Statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"  p-value: {shapiro_pvalue:.4f}\")\n",
    "\n",
    "print(f\"\\nJarque-Bera Test:\")\n",
    "print(f\"  Statistic: {jb_stat:.4f}\")\n",
    "print(f\"  p-value: {jb_pvalue:.4f}\")\n",
    "\n",
    "print(f\"\\nD'Agostino-Pearson Test:\")\n",
    "print(f\"  Statistic: {dagostino_stat:.4f}\")\n",
    "print(f\"  p-value: {dagostino_pvalue:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Skewness: {stats.skew(residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "\n",
    "if shapiro_pvalue > 0.05:\n",
    "    print(f\"\\n✓ Residuals appear normally distributed (Shapiro p={shapiro_pvalue:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Residuals may not be normally distributed (Shapiro p={shapiro_pvalue:.4f})\")\n",
    "    print(\"   Note: With large samples, slight deviations are common.\")\n",
    "    print(\"   Check visual diagnostics for practical significance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual normality checks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0].hist(residuals, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "xmin, xmax = axes[0].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "axes[0].plot(x, stats.norm.pdf(x, residuals.mean(), residuals.std()), 'r-', linewidth=2)\n",
    "axes[0].set_title('Histogram of Residuals')\n",
    "axes[0].set_xlabel('Residuals')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "# KDE plot\n",
    "residuals.plot(kind='kde', ax=axes[2], label='Residuals')\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[2].plot(x, stats.norm.pdf(x, residuals.mean(), residuals.std()), 'r-', label='Normal')\n",
    "axes[2].set_title('Kernel Density Estimation')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Assumption 5: No Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance Inflation Factor (VIF)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(\"=== MULTICOLLINEARITY CHECK (VIF) ===\")\n",
    "print(vif_data)\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  VIF = 1: No correlation\")\n",
    "print(f\"  VIF < 5: Acceptable\")\n",
    "print(f\"  VIF > 5: Moderate multicollinearity\")\n",
    "print(f\"  VIF > 10: High multicollinearity (problematic)\")\n",
    "\n",
    "max_vif = vif_data['VIF'].max()\n",
    "if max_vif < 5:\n",
    "    print(f\"\\n✓ All VIF values < 5: No multicollinearity issues\")\n",
    "elif max_vif < 10:\n",
    "    print(f\"\\n⚠️ Max VIF = {max_vif:.2f}: Moderate multicollinearity\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Max VIF = {max_vif:.2f}: High multicollinearity detected\")\n",
    "    print(\"   Consider: Removing correlated features, PCA, Ridge regression\")\n",
    "\n",
    "# Correlation matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LINEAR REGRESSION ASSUMPTIONS VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compile results\n",
    "results = {\n",
    "    'Linearity': 'Check correlation coefficients above',\n",
    "    'Independence (Durbin-Watson)': f'{dw_statistic:.4f} (ideal ≈ 2)',\n",
    "    'Homoscedasticity (Breusch-Pagan p-value)': f'{bp_pvalue:.4f} (> 0.05 is good)',\n",
    "    'Normality (Shapiro-Wilk p-value)': f'{shapiro_pvalue:.4f} (> 0.05 is good)',\n",
    "    'Multicollinearity (Max VIF)': f'{max_vif:.2f} (< 5 is good)'\n",
    "}\n",
    "\n",
    "print(\"\\n=== ASSUMPTION CHECKS ===\")\n",
    "for assumption, result in results.items():\n",
    "    print(f\"{assumption}: {result}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if dw_statistic < 1.5 or dw_statistic > 2.5:\n",
    "    recommendations.append(\"- Autocorrelation detected: Consider adding lag features or using ARIMA\")\n",
    "\n",
    "if bp_pvalue < 0.05:\n",
    "    recommendations.append(\"- Heteroscedasticity detected: Consider log transformation or WLS\")\n",
    "\n",
    "if shapiro_pvalue < 0.05:\n",
    "    recommendations.append(\"- Non-normal residuals: Consider robust regression or transformation\")\n",
    "\n",
    "if max_vif > 5:\n",
    "    recommendations.append(\"- Multicollinearity detected: Consider removing correlated features\")\n",
    "\n",
    "if abs(skewness) > 1:\n",
    "    recommendations.append(f\"- High skewness ({skewness:.2f}): Consider log transformation of target\")\n",
    "\n",
    "if len(outliers_iqr) > 0:\n",
    "    recommendations.append(f\"- {len(outliers_iqr)} outliers detected: Consider capping or robust methods\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "else:\n",
    "    print(\"✓ All assumptions reasonably satisfied. Linear Regression is appropriate.\")\n",
    "\n",
    "print(\"\\n=== ALTERNATIVE MODELS TO CONSIDER ===\")\n",
    "print(\"If assumptions are violated:\")\n",
    "print(\"  1. Ridge/Lasso Regression - handles multicollinearity\")\n",
    "print(\"  2. Robust Regression - handles outliers and non-normality\")\n",
    "print(\"  3. ARIMA/SARIMA - handles autocorrelation in time series\")\n",
    "print(\"  4. Random Forest - non-parametric, no assumptions required\")\n",
    "print(\"  5. Gradient Boosting - handles complex non-linear patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final diagnostic plots (4-in-1)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "axes[0, 0].scatter(predictions, residuals, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Fitted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Fitted')\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Normal Q-Q')\n",
    "\n",
    "# 3. Scale-Location\n",
    "axes[1, 0].scatter(predictions, np.sqrt(np.abs(residuals)), alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Fitted Values')\n",
    "axes[1, 0].set_ylabel('√|Standardized Residuals|')\n",
    "axes[1, 0].set_title('Scale-Location')\n",
    "\n",
    "# 4. Residuals vs Leverage (Cook's Distance)\n",
    "influence = model.get_influence()\n",
    "leverage = influence.hat_matrix_diag\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "\n",
    "axes[1, 1].scatter(leverage, residuals, alpha=0.5)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Leverage')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residuals vs Leverage')\n",
    "\n",
    "# Highlight high Cook's distance points\n",
    "high_cooks = cooks_d > 4 / len(cooks_d)\n",
    "if high_cooks.sum() > 0:\n",
    "    axes[1, 1].scatter(leverage[high_cooks], residuals.values[high_cooks], \n",
    "                       color='red', s=100, label='High influence')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.suptitle('Regression Diagnostic Plots', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInfluential observations (Cook's D > 4/n): {high_cooks.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
